[{"filename": "get_samples.py", "text": "import argparse\nimport json\nimport random\nimport numpy as np\n\n\ndef get_suffix_position(line: str, start: int) -> int:\n    \"\"\"\n    Get suffix start position from a line.\n    If completing the code inside brackets then return the\n    position of the closing bracket.\n    Else return the end of the line.\n    \"\"\"\n    brackets = 0\n    for i, char in enumerate(line[start:]):\n        if char in \"([{\":\n            brackets += 1\n        elif char in \")]}\":\n            brackets -= 1\n        if brackets < 0:\n            return i + start\n    return len(line) - 1 # -1 not to sample \\n\n\n\ndef get_samples(file_names: list[str], output: str, seed: int, num_samples: int) -> None:\n    \"\"\"\n    Split given files into three parts simulating the user cursor position:\n    the prefix - code before the cursor, the suffix - code after the cursor,\n    and the middle - code that is missing and we assume should be typed next.\n    \"\"\"\n    dataset = []\n    random.seed(seed)\n    for filename in file_names:\n        with open(filename, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n        char_num = np.array([len(line) for line in lines])  # lengts of each line\n        line_end_pos = char_num.cumsum()\n        sampled_idx = random.sample(\n            range(line_end_pos[-1]),  # starting positions of samples\n            min(num_samples, line_end_pos[-1]),\n        )\n        sampled_lines = np.searchsorted(line_end_pos, sampled_idx, side=\"right\")\n        samples = []\n        for line, pos in zip(sampled_lines, sampled_idx):\n            line_pos = len(lines[line]) + pos - line_end_pos[line]  # position of a sample in a line\n            suffix_pos = get_suffix_position(lines[line], int(line_pos))\n            samples.append(\n                {\n                    \"middle\": \"\".join(lines[line][line_pos:suffix_pos]),\n                    \"middle_start\": pos,\n                    \"middle_end\": (\n                        int(line_end_pos[line - 1]) + suffix_pos if line > 0 else suffix_pos\n                    ),\n                }\n            )\n        dataset.append({\"filename\": filename, \"text\": \"\".join(lines), \"samples\": samples})\n    with open(output, \"w\", encoding=\"utf-8\") as jsonfile:\n        json.dump(dataset, jsonfile)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--file_names\",\n        default=[\"get_samples.py\", \"annotate.py\"],\n        type=str,\n        nargs=\"+\",\n        help=\"List of file names\",\n    )\n    parser.add_argument(\"--output\", default=\"dataset.json\", type=str, help=\"Output file name\")\n    parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed\")\n    parser.add_argument(\"--num_samples\", default=25, type=int, help=\"Number of samples per file\")\n    get_samples_args = parser.parse_args()\n    get_samples(**vars(get_samples_args))\n\n\nif __name__ == \"__main__\":\n    main()\n", "samples": [{"middle": "\"Random seed\"", "middle_start": 2619, "middle_end": 2632, "generation_result": "\"Random seed\")\n    parser.add_argument(\"--num_samples\", default=25, type=int, help", "metrics": {"chrf": {"score": 43.67136488885444, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.27368421052631575, "lcs": 13, "lcs_normalized": 1.0, "chrf++": {"score": 41.52567873478503, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Genrated correctly but also a few lines from suffix", "label": 0.6}, {"middle": "       brackets -= 1", "middle_start": 456, "middle_end": 476, "generation_result": "   brackets -= 1\n        else:\n            return i + start\n    return len(line) - 1  # -", "metrics": {"chrf": {"score": 53.270530102142786, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.29357798165137616, "lcs": 16, "lcs_normalized": 0.8, "chrf++": {"score": 54.314284672298655, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Did not stop at the correct place", "label": 0.6}, {"middle": "t: int", "middle_start": 102, "middle_end": 108, "generation_result": "n: int) -> int:\n    \"\"\"\n    Get suffix position from a line.\n    If completing the code inside brackets", "metrics": {"chrf": {"score": 13.448568482017237, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.11009174311926606, "lcs": 5, "lcs_normalized": 0.8333333333333334, "chrf++": {"score": 16.535887760266082, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Almost correct + code from suffix", "label": 0.6}, {"middle": " in lines", "middle_start": 1126, "middle_end": 1135, "generation_result": "                in lines])\n        line_start_pos = char_num.cumsum()\n        line_end_pos =", "metrics": {"chrf": {"score": 32.0972140445625, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.17821782178217827, "lcs": 9, "lcs_normalized": 1.0, "chrf++": {"score": 28.353337840004546, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "A lot of empty space and did not stop at the correct place", "label": 0.5}, {"middle": "en(filename, \"r\", encoding=\"utf-8\") as f:", "middle_start": 1003, "middle_end": 1044, "generation_result": "en(filename, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n        char", "metrics": {"chrf": {"score": 88.22000199134449, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.6356589147286822, "lcs": 41, "lcs_normalized": 1.0, "chrf++": {"score": 88.5362344888148, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Generated also a few lines from suffix, but other than that quite good", "label": 0.7}, {"middle": "\"\"", "middle_start": 914, "middle_end": 916, "generation_result": "\"\"\n    with open(file_names[0], \"r\", encoding=\"utf-8\") as f:\n        lines", "metrics": {"chrf": {"score": 12.363470873786406, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.052631578947368474, "lcs": 2, "lcs_normalized": 1.0, "chrf++": {"score": 28.25538973469776, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Did not stop at the correct place", "label": 0.6}, {"middle": "ple \\n", "middle_start": 571, "middle_end": 577, "generation_result": "ple", "metrics": {"chrf": {"score": 53.34987593052109, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.6666666666666667, "lcs": 3, "lcs_normalized": 0.5, "chrf++": {"score": 49.71857410881801, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Almost good", "label": 0.7}, {"middle": "= 1", "middle_start": 419, "middle_end": 422, "generation_result": "        brackets + 1\n        elif char in \")]}\":\n            brackets -= 1\n        if brackets < 0:", "metrics": {"chrf": {"score": 13.47414420975965, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.05882352941176472, "lcs": 3, "lcs_normalized": 1.0, "chrf++": {"score": 24.238665257056315, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.0}, {"middle": "s()", "middle_start": 2771, "middle_end": 2774, "generation_result": "\n    get_samples_args.func(get_samples_args)\n\n    get_samples(**vars(get_samples", "metrics": {"chrf": {"score": 8.394833948339485, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.07228915662650603, "lcs": 2, "lcs_normalized": 0.6666666666666666, "chrf++": {"score": 21.957924934257704, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.0}, {"middle": "def main():", "middle_start": 2233, "middle_end": 2244, "generation_result": "def main():\n    get_samples(**vars(get_samples_args))", "metrics": {"chrf": {"score": 50.11590024780767, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.34375, "lcs": 11, "lcs_normalized": 1.0, "chrf++": {"score": 64.08188091640811, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Generated also a line from suffix (and not the first one)", "label": 0.6}, {"middle": "line[start:]", "middle_start": 356, "middle_end": 368, "generation_result": "        line.strip().split(", "metrics": {"chrf": {"score": 19.85842396298981, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.41025641025641024, "lcs": 4, "lcs_normalized": 0.3333333333333333, "chrf++": {"score": 14.89381797224236, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Not completely correct and a lot of empty space", "label": 0.4}, {"middle": "+\",", "middle_start": 2418, "middle_end": 2421, "generation_result": "--file_names\",\n        help=\"List of file names\",\n    )\n    parser.add_argument(\"--output\", default", "metrics": {"chrf": {"score": 5.81737416526932, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.039215686274509776, "lcs": 2, "lcs_normalized": 0.6666666666666666, "chrf++": {"score": 9.940932451206127, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.0}, {"middle": "", "middle_start": 1728, "middle_end": 1728, "generation_result": "   ", "metrics": {"chrf": {"score": 0.0, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.0, "lcs": 0, "lcs_normalized": 0, "chrf++": {"score": 0.0, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Empty middle example", "label": 0.0}, {"middle": "Get suffix start position from a line.", "middle_start": 130, "middle_end": 168, "generation_result": "   Get the suffix position of a line.\n    \"\"\"\n    return len(line) - start\n\n\ndef get_suffix", "metrics": {"chrf": {"score": 44.685799120988456, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.4496124031007752, "lcs": 10, "lcs_normalized": 0.2631578947368421, "chrf++": {"score": 45.2887326517682, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Ended the comment earlier", "label": 0.4}, {"middle": "\"\"\"", "middle_start": 122, "middle_end": 125, "generation_result": "\"\"\"\n    Get suffix position from a line.\n    If completing the code inside brackets then return the\n    position of the closing", "metrics": {"chrf": {"score": 9.67796785274595, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.0461538461538461, "lcs": 3, "lcs_normalized": 1.0, "chrf++": {"score": 17.350444493099772, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Generated correctly + a few lines from suffix", "label": 0.6}, {"middle": "har in \"([{\":", "middle_start": 383, "middle_end": 396, "generation_result": "            in \"([{\":\n                brackets += 1\n            elif c in \")]}\":\n            brackets -= 1\n        if", "metrics": {"chrf": {"score": 37.23833732630713, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.15384615384615385, "lcs": 10, "lcs_normalized": 0.7692307692307693, "chrf++": {"score": 38.10980846257641, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Incorrect variable name + did not stop at the correct place", "label": 0.5}, {"middle": "e typed next.", "middle_start": 895, "middle_end": 908, "generation_result": "e\n    the cursor position.\n    \"\"\"\n    dataset = []\n    random.seed(seed)\n    for filename in file_", "metrics": {"chrf": {"score": 8.331044584821019, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.1607142857142857, "lcs": 2, "lcs_normalized": 0.15384615384615385, "chrf++": {"score": 9.98194874588948, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Not completely correct + a few lines from suffix", "label": 0.4}, {"middle": "ed", "middle_start": 952, "middle_end": 954, "generation_result": "lf.seed)\n    random.shuffle(file_names)\n    for filename in file_names:\n        with open(", "metrics": {"chrf": {"score": 10.32157676348548, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.04347826086956519, "lcs": 2, "lcs_normalized": 1.0, "chrf++": {"score": 6.881051175656985, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.1}, {"middle": "\"filename\": filename, \"text\": \"\".join(lines), \"samples\": samples", "middle_start": 2069, "middle_end": 2133, "generation_result": "\"samples\": samples, \"seed\": seed, \"num_samples\": num_samples", "metrics": {"chrf": {"score": 35.46811521841005, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.6129032258064516, "lcs": 18, "lcs_normalized": 0.28125, "chrf++": {"score": 37.06176139621359, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Good guess", "label": 0.6}, {"middle": "  parser.add_argument(\"--output\", default=\"dataset.json\", type=str, help=\"Output file name\")", "middle_start": 2465, "middle_end": 2557, "generation_result": "parser.add_argument(\"--output\", default=\"annotate.json\", type=str, help=\"Output file\")\n    parser", "metrics": {"chrf": {"score": 83.90143759717384, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.8783068783068784, "lcs": 41, "lcs_normalized": 0.44565217391304346, "chrf++": {"score": 77.23404899711012, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Almost perfect", "label": 0.9}, {"middle": "", "middle_start": 108, "middle_end": 108, "generation_result": "= 0, end: int = 0) -> int:\n    \"\"\"\n    Get suffix position from a line.\n   ", "metrics": {"chrf": {"score": 0.0, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.0, "lcs": 0, "lcs_normalized": 0, "chrf++": {"score": 0.0, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Empty middle example, shouldn't have generated anything, but I like the end and default value suggestion", "label": 0.6}, {"middle": "d_argument(", "middle_start": 2298, "middle_end": 2309, "generation_result": "d = True\n    parser.add_argument(\"--file_names\", default=[\"get_samples.py\", \"annotate.", "metrics": {"chrf": {"score": 39.06052113208212, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.22680412371134018, "lcs": 11, "lcs_normalized": 1.0, "chrf++": {"score": 29.295390849061597, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.0}, {"middle": "fter the cursor,", "middle_start": 814, "middle_end": 830, "generation_result": "nd the middle - code that is missing and we assume should be typed next.\n    \"\"\"\n    dataset = []\n    random.", "metrics": {"chrf": {"score": 8.185981353634734, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.17600000000000005, "lcs": 5, "lcs_normalized": 0.3125, "chrf++": {"score": 7.702857309855009, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Just generated from suffix", "label": 0.0}, {"middle": "num_samples\", default=25, type=int, help=\"Number of samples per file\"", "middle_start": 2661, "middle_end": 2730, "generation_result": "              \", type=int, help=\"Random seed\")\n    parser.add_argument(\"--num_samples\", default=1", "metrics": {"chrf": {"score": 55.99123401668189, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.40963855421686746, "lcs": 22, "lcs_normalized": 0.3188405797101449, "chrf++": {"score": 49.89549538825042, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "It tried", "label": 0.1}, {"middle": "", "middle_start": 2232, "middle_end": 2232, "generation_result": "dataset = {\n            \"filename\": filename,\n            \"text\": \"\".join(lines),\n            \"samples\": dataset,\n       ", "metrics": {"chrf": {"score": 0.0, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.0, "lcs": 0, "lcs_normalized": 0, "chrf++": {"score": 0.0, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Shouldn't have suggested anything", "label": 0.0}]}, {"filename": "annotate.py", "text": "import argparse\nimport json\nimport os\n\n\nCOLOR_MIDDLE = \"\\033[96m\"\nCOLOR_GENERATED = \"\\033[92m\"\nCPLOR_END = \"\\033[0m\"\n\n\ndef input_or_edit(sample: dict, key: str, val_type: type) -> None:\n    \"\"\"\n    Input a value of a key in a sample or give a possibility to edit if exists\n    \"\"\"\n    if sample.get(key, None) is not None:\n        print(\"Previous value is:\", sample[key])\n        while True:\n            try:\n                user_input = input(key + \": Input new vaue or press enter to keep: \")\n                if user_input != \"\":\n                    sample[key] = val_type(user_input)\n                break\n            except ValueError:\n                print(\"Incorrect value type, enter again\")\n    else:\n        while True:\n            try:\n                user_input = input(key + \": \")\n                sample[key] = val_type(user_input)\n                break\n            except ValueError:\n                print(\"Incorrect value type, enter again: \")\n\n\ndef annotate(file_name: str, output: str) -> None:\n    \"\"\"\n    For each sample display code with middle written in COLOR_MIDDLE,\n    generation result (if exists) written in COLOR_GENERATED\n    and with corresponding metrics.\n    Annotations and labels should be inserted for each sample\n    \"\"\"\n    with open(file_name, \"r\", encoding=\"utf-8\") as jsonfile:\n        dataset = json.load(jsonfile)\n\n    for code in dataset:\n        for sample in code[\"samples\"]:\n            os.system(\"cls\" if os.name == \"nt\" else \"clear\")\n            print(code[\"text\"][: sample[\"middle_start\"]], end=\"\")\n            print(COLOR_MIDDLE + sample[\"middle\"] + CPLOR_END, end=\"\")\n            if sample.get(\"generation_result\", None) is not None:\n                print(COLOR_GENERATED + sample[\"generation_result\"] + CPLOR_END)\n            print(code[\"text\"][sample[\"middle_end\"]:])\n            if sample.get(\"metrics\", None) is not None:\n                for metric, value in sample[\"metrics\"].items():\n                    print(metric, value, sep=\": \")\n\n            input_or_edit(sample, \"annotations\", str)\n            input_or_edit(sample, \"label\", float)\n\n    with open(output, \"w\", encoding=\"utf-8\") as jsonfile:\n        json.dump(dataset, jsonfile)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--file_name\", default=\"output.json\", type=str, help=\"Dataset file name\")\n    parser.add_argument(\"--output\", default=\"annotated.json\", type=str, help=\"Output file name\")\n    annotate(**vars(parser.parse_args()))\n\n\nif __name__ == \"__main__\":\n    main()\n", "samples": [{"middle": "TED + sample[\"generation_result\"] + CPLOR_END", "middle_start": 1718, "middle_end": 1763, "generation_result": "                 + CPLOR_END, end=\"\")\n                print(code[\"text\"][sample[\"middle_start\"]:sample[\"", "metrics": {"chrf": {"score": 36.939671408091016, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.24161073825503354, "lcs": 12, "lcs_normalized": 0.26666666666666666, "chrf++": {"score": 34.27838913443134, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.1}, {"middle": "           print(\"Incorrect value type, enter again: \")", "middle_start": 902, "middle_end": 957, "generation_result": "   print(\"Incorrect value type, enter again\")\n\n\ndef annotate_with_metrics(file_name: str,", "metrics": {"chrf": {"score": 77.37605543211346, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.625, "lcs": 43, "lcs_normalized": 0.7818181818181819, "chrf++": {"score": 73.40557290974368, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "I even like the suggestion more than my own code (without :), but did not stop at the right place again", "label": 0.7}, {"middle": "le.get(\"metrics\", None) is not None:", "middle_start": 1839, "middle_end": 1875, "generation_result": "le[\"metrics\"]:\n               ", "metrics": {"chrf": {"score": 27.15498673717553, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.4545454545454546, "lcs": 9, "lcs_normalized": 0.25, "chrf++": {"score": 22.23285655117024, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Might not work, but ok", "label": 0.7}, {"middle": ", help=\"Output file name\"", "middle_start": 2413, "middle_end": 2438, "generation_result": "              help=\"Output file name\"", "metrics": {"chrf": {"score": 95.82236258705089, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.7741935483870968, "lcs": 24, "lcs_normalized": 0.96, "chrf++": {"score": 92.25192773637751, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "No coma and too much empty space", "label": 0.6}, {"middle": "_GENERATED", "middle_start": 1139, "middle_end": 1149, "generation_result": "\n    \"\"\"\n    with open(file_name, \"r\", encoding=\"utf-8\") as jsonfile:\n        dataset", "metrics": {"chrf": {"score": 0.825082508250825, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.021052631578947323, "lcs": 1, "lcs_normalized": 0.1, "chrf++": {"score": 0.6188118811881188, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Ended the comment earlier and continued with code from suffix", "label": 0.0}, {"middle": "n", "middle_start": 26, "middle_end": 27, "generation_result": "n\nimport json\nimport os\n", "metrics": {"chrf": {"score": 21.73913043478261, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.07999999999999996, "lcs": 1, "lcs_normalized": 1.0, "chrf++": {"score": 41.95804195804195, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "imported json and os twice", "label": 0.6}, {"middle": "   print(\"Incorrect value type, enter again\")", "middle_start": 653, "middle_end": 698, "generation_result": "   print(\"Incorrect value type, enter again: \")\n\n\ndef val_type(val: str) -> type:", "metrics": {"chrf": {"score": 82.64029780182707, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.7142857142857143, "lcs": 43, "lcs_normalized": 0.9555555555555556, "chrf++": {"score": 76.65623128071873, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Did not stop at the correct place", "label": 0.6}, {"middle": "\"generation_result\"", "middle_start": 1731, "middle_end": 1750, "generation_result": "\"generation_result\"] + CPLOR_END, end=\"\")\n            if sample.get(\"metrics\", None) is", "metrics": {"chrf": {"score": 62.674414143976385, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.3584905660377359, "lcs": 19, "lcs_normalized": 1.0, "chrf++": {"score": 47.00581060798229, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Did not stop at the correct place", "label": 0.6}, {"middle": "sample in code[\"samples\"]:", "middle_start": 1393, "middle_end": 1419, "generation_result": "    sample = code[\"sample\"]\n            sample[\"middle\"] = sample[\"middle\"].replace(\"\\n\", COLOR_GENERATED)", "metrics": {"chrf": {"score": 41.20982271076704, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.3484848484848485, "lcs": 13, "lcs_normalized": 0.5, "chrf++": {"score": 33.273077029781184, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.0}, {"middle": "R_GENERATED", "middle_start": 1138, "middle_end": 1149, "generation_result": "\n    \"\"\"\n    with open(file_name, \"r\", encoding=\"utf-8\") as jsonfile:\n        dataset", "metrics": {"chrf": {"score": 0.7936507936507936, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.02083333333333337, "lcs": 1, "lcs_normalized": 0.09090909090909091, "chrf++": {"score": 0.6802721088435374, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Did not complete the comment", "label": 0.0}, {"middle": "or:", "middle_start": 636, "middle_end": 639, "generation_result": "   ", "metrics": {"chrf": {"score": 0.0, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.0, "lcs": 0, "lcs_normalized": 0.0, "chrf++": {"score": 0.0, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Did not complete the example", "label": 0.0}, {"middle": "ept ValueError:", "middle_start": 881, "middle_end": 896, "generation_result": "ept ValueError\n            except KeyError:\n                print(\"Incorrect key type, enter again\")\n\n\ndef annotate_with_metrics", "metrics": {"chrf": {"score": 43.653128308867615, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.2097902097902098, "lcs": 14, "lcs_normalized": 0.9333333333333333, "chrf++": {"score": 43.23420199268079, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "No : and a lot of garbage", "label": 0.4}, {"middle": "t:", "middle_start": 1378, "middle_end": 1380, "generation_result": "t[\"samples\"]:\n        print(code[\"text\"][: code[\"middle_start\"]], end=\"\")\n        print(COLOR_", "metrics": {"chrf": {"score": 6.097560975609757, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.04166666666666663, "lcs": 1, "lcs_normalized": 0.5, "chrf++": {"score": 11.8606998654105, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.1}, {"middle": "       user_input = input(key + \": Input new vaue or press enter to keep: \")", "middle_start": 418, "middle_end": 494, "generation_result": "   user_input = input(key + \": \")", "metrics": {"chrf": {"score": 44.97496414635438, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.6055045871559632, "lcs": 31, "lcs_normalized": 0.40789473684210525, "chrf++": {"score": 46.47838530655232, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Good", "label": 0.8}, {"middle": " while True:", "middle_start": 379, "middle_end": 391, "generation_result": "     print(\"New value is:\", sample[key])\n                        print(\"Previous value type:\", type(sample[key]))\n                       ", "metrics": {"chrf": {"score": 7.933197173572546, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.13422818791946312, "lcs": 2, "lcs_normalized": 0.16666666666666666, "chrf++": {"score": 5.94989788017941, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.0}, {"middle": "   print(COLOR_MIDDLE + sample[\"middle\"] + CPLOR_END, end=\"\")", "middle_start": 1556, "middle_end": 1617, "generation_result": "   # print(sample[\"text\"])\n            if sample.get(\"annotations\", None) is not None:\n                print(", "metrics": {"chrf": {"score": 20.956776797002398, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.3176470588235294, "lcs": 9, "lcs_normalized": 0.14754098360655737, "chrf++": {"score": 18.227564931170594, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Incorrect", "label": 0.1}, {"middle": "        try:", "middle_start": 396, "middle_end": 408, "generation_result": "print(COLOR_GENERATED + sample[\"generation_result\"] + CPLOR_END)\n            print(code[\"text", "metrics": {"chrf": {"score": 2.717391304347826, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.17142857142857137, "lcs": 8, "lcs_normalized": 0.6666666666666666, "chrf++": {"score": 1.8115942028985508, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Just generated code from suffix", "label": 0.0}, {"middle": "e \"clear\"", "middle_start": 1470, "middle_end": 1479, "generation_result": "e \"clear\")\n            sample[\"middle\"] = sample[\"middle_start\"]\n            sample[\"generation_result\"] = sample", "metrics": {"chrf": {"score": 26.961551098950732, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.14754098360655743, "lcs": 9, "lcs_normalized": 1.0, "chrf++": {"score": 23.599824049975325, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Did not stop at the correct place", "label": 0.6}, {"middle": "\"samples\"", "middle_start": 1408, "middle_end": 1417, "generation_result": "\"samples\"", "metrics": {"chrf": {"score": 100.0, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 1.0}, "syntactic_correctness": 1, "levenshtein": 1.0, "lcs": 9, "lcs_normalized": 1.0, "chrf++": {"score": 100.0, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Perfect", "label": 1.0}, {"middle": "_args()", "middle_start": 2472, "middle_end": 2479, "generation_result": "      ", "metrics": {"chrf": {"score": 0.0, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.0, "lcs": 0, "lcs_normalized": 0.0, "chrf++": {"score": 0.0, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Did not complte the call", "label": 0.0}, {"middle": "DDLE,", "middle_start": 1083, "middle_end": 1088, "generation_result": "\n    and with corresponding metrics.\n    Annotations and labels should be inserted for each sample\n    \"\"\"\n    with open(file_", "metrics": {"chrf": {"score": 0.0, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.0, "lcs": 0, "lcs_normalized": 0.0, "chrf++": {"score": 0.0, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Just code from suffix", "label": 0.0}, {"middle": "-> None:", "middle_start": 177, "middle_end": 185, "generation_result": "        # type: (dict, str, type) -> None\n    \"\"\"\n    Input a value of a key in a sample", "metrics": {"chrf": {"score": 25.93607691724835, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.14583333333333337, "lcs": 7, "lcs_normalized": 0.875, "chrf++": {"score": 30.03597158988273, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Bad", "label": 0.0}, {"middle": "           for metric, value in sample[\"metrics\"].items():", "middle_start": 1881, "middle_end": 1939, "generation_result": "               print(metric, sample[\"metrics\"], sep=\": \")\n\n            input_or_edit(sample, \"annotations\",", "metrics": {"chrf": {"score": 47.278229611633414, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.509090909090909, "lcs": 18, "lcs_normalized": 0.3103448275862069, "chrf++": {"score": 38.58402863466717, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Gnerated slightly changed code from suffix", "label": 0.0}, {"middle": "f main():", "middle_start": 2196, "middle_end": 2205, "generation_result": "f main():\n    \"\"\"\n    Main function\n    \"\"\"", "metrics": {"chrf": {"score": 59.89038349374645, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 1, "levenshtein": 0.34615384615384615, "lcs": 9, "lcs_normalized": 1.0, "chrf++": {"score": 61.91794614791492, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Perfect", "label": 1.0}, {"middle": "if user_input != \"\":", "middle_start": 511, "middle_end": 531, "generation_result": "    if user_input.lower() == val_type.lower():\n                    break\n            except ValueError:\n                print(\"Incorrect", "metrics": {"chrf": {"score": 37.38917009759904, "char_order": 6, "word_order": 0, "beta": 2}, "exact_match": {"exact_match": 0.0}, "syntactic_correctness": 0, "levenshtein": 0.21794871794871795, "lcs": 13, "lcs_normalized": 0.65, "chrf++": {"score": 34.248044305203074, "char_order": 6, "word_order": 2, "beta": 2}}, "annotations": "Incorrect", "label": 0.0}]}]